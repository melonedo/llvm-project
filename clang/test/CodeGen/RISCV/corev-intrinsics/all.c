// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// RUN: %clang_cc1 -triple riscv32 -target-feature +xcorevsimd -emit-llvm %s -o - \
// RUN:     | FileCheck %s

#include <stdint.h>

// CHECK-LABEL: @test_add_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.add.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_add_h(uint32_t a, uint32_t b) {
	return __builtin_corev_add_h(a, b);
}

// CHECK-LABEL: @test_add_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.add.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_add_b(uint32_t a, uint32_t b) {
	return __builtin_corev_add_b(a, b);
}

// CHECK-LABEL: @test_add_sc_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.add.sc.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_add_sc_h(uint32_t a, uint32_t b) {
	return __builtin_corev_add_sc_h(a, b);
}

// CHECK-LABEL: @test_add_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.add.sc.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_add_sci_h(uint32_t a) {
	return __builtin_corev_add_sc_h(a, 5);
}

// CHECK-LABEL: @test_add_sc_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.add.sc.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_add_sc_b(uint32_t a, uint32_t b) {
	return __builtin_corev_add_sc_b(a, b);
}

// CHECK-LABEL: @test_add_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.add.sc.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_add_sci_b(uint32_t a) {
	return __builtin_corev_add_sc_b(a, 5);
}

// CHECK-LABEL: @test_sub_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.sub.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_sub_h(uint32_t a, uint32_t b) {
	return __builtin_corev_sub_h(a, b);
}

// CHECK-LABEL: @test_sub_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.sub.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_sub_b(uint32_t a, uint32_t b) {
	return __builtin_corev_sub_b(a, b);
}

// CHECK-LABEL: @test_sub_sc_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.sub.sc.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_sub_sc_h(uint32_t a, uint32_t b) {
	return __builtin_corev_sub_sc_h(a, b);
}

// CHECK-LABEL: @test_sub_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.sub.sc.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_sub_sci_h(uint32_t a) {
	return __builtin_corev_sub_sc_h(a, 5);
}

// CHECK-LABEL: @test_sub_sc_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.sub.sc.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_sub_sc_b(uint32_t a, uint32_t b) {
	return __builtin_corev_sub_sc_b(a, b);
}

// CHECK-LABEL: @test_sub_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.sub.sc.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_sub_sci_b(uint32_t a) {
	return __builtin_corev_sub_sc_b(a, 5);
}

// CHECK-LABEL: @test_avg_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.avg.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_avg_h(uint32_t a, uint32_t b) {
	return __builtin_corev_avg_h(a, b);
}

// CHECK-LABEL: @test_avg_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.avg.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_avg_b(uint32_t a, uint32_t b) {
	return __builtin_corev_avg_b(a, b);
}

// CHECK-LABEL: @test_avg_sc_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.avg.sc.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_avg_sc_h(uint32_t a, uint32_t b) {
	return __builtin_corev_avg_sc_h(a, b);
}

// CHECK-LABEL: @test_avg_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.avg.sc.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_avg_sci_h(uint32_t a) {
	return __builtin_corev_avg_sc_h(a, 5);
}

// CHECK-LABEL: @test_avg_sc_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.avg.sc.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_avg_sc_b(uint32_t a, uint32_t b) {
	return __builtin_corev_avg_sc_b(a, b);
}

// CHECK-LABEL: @test_avg_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.avg.sc.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_avg_sci_b(uint32_t a) {
	return __builtin_corev_avg_sc_b(a, 5);
}

// CHECK-LABEL: @test_avgu_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.avgu.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_avgu_h(uint32_t a, uint32_t b) {
	return __builtin_corev_avgu_h(a, b);
}

// CHECK-LABEL: @test_avgu_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.avgu.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_avgu_b(uint32_t a, uint32_t b) {
	return __builtin_corev_avgu_b(a, b);
}

// CHECK-LABEL: @test_avgu_sc_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.avgu.sc.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_avgu_sc_h(uint32_t a, uint32_t b) {
	return __builtin_corev_avgu_sc_h(a, b);
}

// CHECK-LABEL: @test_avgu_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.avgu.sc.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_avgu_sci_h(uint32_t a) {
	return __builtin_corev_avgu_sc_h(a, 5);
}

// CHECK-LABEL: @test_avgu_sc_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.avgu.sc.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_avgu_sc_b(uint32_t a, uint32_t b) {
	return __builtin_corev_avgu_sc_b(a, b);
}

// CHECK-LABEL: @test_avgu_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.avgu.sc.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_avgu_sci_b(uint32_t a) {
	return __builtin_corev_avgu_sc_b(a, 5);
}

// CHECK-LABEL: @test_min_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.min.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_min_h(uint32_t a, uint32_t b) {
	return __builtin_corev_min_h(a, b);
}

// CHECK-LABEL: @test_min_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.min.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_min_b(uint32_t a, uint32_t b) {
	return __builtin_corev_min_b(a, b);
}

// CHECK-LABEL: @test_min_sc_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.min.sc.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_min_sc_h(uint32_t a, uint32_t b) {
	return __builtin_corev_min_sc_h(a, b);
}

// CHECK-LABEL: @test_min_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.min.sc.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_min_sci_h(uint32_t a) {
	return __builtin_corev_min_sc_h(a, 5);
}

// CHECK-LABEL: @test_min_sc_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.min.sc.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_min_sc_b(uint32_t a, uint32_t b) {
	return __builtin_corev_min_sc_b(a, b);
}

// CHECK-LABEL: @test_min_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.min.sc.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_min_sci_b(uint32_t a) {
	return __builtin_corev_min_sc_b(a, 5);
}

// CHECK-LABEL: @test_minu_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.minu.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_minu_h(uint32_t a, uint32_t b) {
	return __builtin_corev_minu_h(a, b);
}

// CHECK-LABEL: @test_minu_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.minu.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_minu_b(uint32_t a, uint32_t b) {
	return __builtin_corev_minu_b(a, b);
}

// CHECK-LABEL: @test_minu_sc_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.minu.sc.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_minu_sc_h(uint32_t a, uint32_t b) {
	return __builtin_corev_minu_sc_h(a, b);
}

// CHECK-LABEL: @test_minu_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.minu.sc.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_minu_sci_h(uint32_t a) {
	return __builtin_corev_minu_sc_h(a, 5);
}

// CHECK-LABEL: @test_minu_sc_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.minu.sc.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_minu_sc_b(uint32_t a, uint32_t b) {
	return __builtin_corev_minu_sc_b(a, b);
}

// CHECK-LABEL: @test_minu_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.minu.sc.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_minu_sci_b(uint32_t a) {
	return __builtin_corev_minu_sc_b(a, 5);
}

// CHECK-LABEL: @test_max_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.max.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_max_h(uint32_t a, uint32_t b) {
	return __builtin_corev_max_h(a, b);
}

// CHECK-LABEL: @test_max_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.max.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_max_b(uint32_t a, uint32_t b) {
	return __builtin_corev_max_b(a, b);
}

// CHECK-LABEL: @test_max_sc_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.max.sc.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_max_sc_h(uint32_t a, uint32_t b) {
	return __builtin_corev_max_sc_h(a, b);
}

// CHECK-LABEL: @test_max_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.max.sc.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_max_sci_h(uint32_t a) {
	return __builtin_corev_max_sc_h(a, 5);
}

// CHECK-LABEL: @test_max_sc_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.max.sc.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_max_sc_b(uint32_t a, uint32_t b) {
	return __builtin_corev_max_sc_b(a, b);
}

// CHECK-LABEL: @test_max_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.max.sc.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_max_sci_b(uint32_t a) {
	return __builtin_corev_max_sc_b(a, 5);
}

// CHECK-LABEL: @test_maxu_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.maxu.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_maxu_h(uint32_t a, uint32_t b) {
	return __builtin_corev_maxu_h(a, b);
}

// CHECK-LABEL: @test_maxu_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.maxu.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_maxu_b(uint32_t a, uint32_t b) {
	return __builtin_corev_maxu_b(a, b);
}

// CHECK-LABEL: @test_maxu_sc_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.maxu.sc.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_maxu_sc_h(uint32_t a, uint32_t b) {
	return __builtin_corev_maxu_sc_h(a, b);
}

// CHECK-LABEL: @test_maxu_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.maxu.sc.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_maxu_sci_h(uint32_t a) {
	return __builtin_corev_maxu_sc_h(a, 5);
}

// CHECK-LABEL: @test_maxu_sc_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.maxu.sc.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_maxu_sc_b(uint32_t a, uint32_t b) {
	return __builtin_corev_maxu_sc_b(a, b);
}

// CHECK-LABEL: @test_maxu_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.maxu.sc.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_maxu_sci_b(uint32_t a) {
	return __builtin_corev_maxu_sc_b(a, 5);
}

// CHECK-LABEL: @test_srl_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.srl.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_srl_h(uint32_t a, uint32_t b) {
	return __builtin_corev_srl_h(a, b);
}

// CHECK-LABEL: @test_srl_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.srl.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_srl_b(uint32_t a, uint32_t b) {
	return __builtin_corev_srl_b(a, b);
}

// CHECK-LABEL: @test_srl_sc_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.srl.sc.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_srl_sc_h(uint32_t a, uint32_t b) {
	return __builtin_corev_srl_sc_h(a, b);
}

// CHECK-LABEL: @test_srl_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.srl.sc.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_srl_sci_h(uint32_t a) {
	return __builtin_corev_srl_sc_h(a, 5);
}

// CHECK-LABEL: @test_srl_sc_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.srl.sc.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_srl_sc_b(uint32_t a, uint32_t b) {
	return __builtin_corev_srl_sc_b(a, b);
}

// CHECK-LABEL: @test_srl_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.srl.sc.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_srl_sci_b(uint32_t a) {
	return __builtin_corev_srl_sc_b(a, 5);
}

// CHECK-LABEL: @test_sra_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.sra.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_sra_h(uint32_t a, uint32_t b) {
	return __builtin_corev_sra_h(a, b);
}

// CHECK-LABEL: @test_sra_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.sra.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_sra_b(uint32_t a, uint32_t b) {
	return __builtin_corev_sra_b(a, b);
}

// CHECK-LABEL: @test_sra_sc_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.sra.sc.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_sra_sc_h(uint32_t a, uint32_t b) {
	return __builtin_corev_sra_sc_h(a, b);
}

// CHECK-LABEL: @test_sra_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.sra.sc.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_sra_sci_h(uint32_t a) {
	return __builtin_corev_sra_sc_h(a, 5);
}

// CHECK-LABEL: @test_sra_sc_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.sra.sc.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_sra_sc_b(uint32_t a, uint32_t b) {
	return __builtin_corev_sra_sc_b(a, b);
}

// CHECK-LABEL: @test_sra_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.sra.sc.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_sra_sci_b(uint32_t a) {
	return __builtin_corev_sra_sc_b(a, 5);
}

// CHECK-LABEL: @test_sll_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.sll.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_sll_h(uint32_t a, uint32_t b) {
	return __builtin_corev_sll_h(a, b);
}

// CHECK-LABEL: @test_sll_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.sll.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_sll_b(uint32_t a, uint32_t b) {
	return __builtin_corev_sll_b(a, b);
}

// CHECK-LABEL: @test_sll_sc_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.sll.sc.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_sll_sc_h(uint32_t a, uint32_t b) {
	return __builtin_corev_sll_sc_h(a, b);
}

// CHECK-LABEL: @test_sll_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.sll.sc.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_sll_sci_h(uint32_t a) {
	return __builtin_corev_sll_sc_h(a, 5);
}

// CHECK-LABEL: @test_sll_sc_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.sll.sc.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_sll_sc_b(uint32_t a, uint32_t b) {
	return __builtin_corev_sll_sc_b(a, b);
}

// CHECK-LABEL: @test_sll_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.sll.sc.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_sll_sci_b(uint32_t a) {
	return __builtin_corev_sll_sc_b(a, 5);
}

// CHECK-LABEL: @test_or_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.or.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_or_h(uint32_t a, uint32_t b) {
	return __builtin_corev_or_h(a, b);
}

// CHECK-LABEL: @test_or_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.or.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_or_b(uint32_t a, uint32_t b) {
	return __builtin_corev_or_b(a, b);
}

// CHECK-LABEL: @test_or_sc_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.or.sc.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_or_sc_h(uint32_t a, uint32_t b) {
	return __builtin_corev_or_sc_h(a, b);
}

// CHECK-LABEL: @test_or_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.or.sc.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_or_sci_h(uint32_t a) {
	return __builtin_corev_or_sc_h(a, 5);
}

// CHECK-LABEL: @test_or_sc_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.or.sc.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_or_sc_b(uint32_t a, uint32_t b) {
	return __builtin_corev_or_sc_b(a, b);
}

// CHECK-LABEL: @test_or_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.or.sc.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_or_sci_b(uint32_t a) {
	return __builtin_corev_or_sc_b(a, 5);
}

// CHECK-LABEL: @test_xor_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.xor.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_xor_h(uint32_t a, uint32_t b) {
	return __builtin_corev_xor_h(a, b);
}

// CHECK-LABEL: @test_xor_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.xor.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_xor_b(uint32_t a, uint32_t b) {
	return __builtin_corev_xor_b(a, b);
}

// CHECK-LABEL: @test_xor_sc_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.xor.sc.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_xor_sc_h(uint32_t a, uint32_t b) {
	return __builtin_corev_xor_sc_h(a, b);
}

// CHECK-LABEL: @test_xor_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.xor.sc.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_xor_sci_h(uint32_t a) {
	return __builtin_corev_xor_sc_h(a, 5);
}

// CHECK-LABEL: @test_xor_sc_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.xor.sc.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_xor_sc_b(uint32_t a, uint32_t b) {
	return __builtin_corev_xor_sc_b(a, b);
}

// CHECK-LABEL: @test_xor_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.xor.sc.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_xor_sci_b(uint32_t a) {
	return __builtin_corev_xor_sc_b(a, 5);
}

// CHECK-LABEL: @test_and_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.and.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_and_h(uint32_t a, uint32_t b) {
	return __builtin_corev_and_h(a, b);
}

// CHECK-LABEL: @test_and_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.and.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_and_b(uint32_t a, uint32_t b) {
	return __builtin_corev_and_b(a, b);
}

// CHECK-LABEL: @test_and_sc_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.and.sc.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_and_sc_h(uint32_t a, uint32_t b) {
	return __builtin_corev_and_sc_h(a, b);
}

// CHECK-LABEL: @test_and_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.and.sc.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_and_sci_h(uint32_t a) {
	return __builtin_corev_and_sc_h(a, 5);
}

// CHECK-LABEL: @test_and_sc_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.and.sc.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_and_sc_b(uint32_t a, uint32_t b) {
	return __builtin_corev_and_sc_b(a, b);
}

// CHECK-LABEL: @test_and_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.and.sc.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_and_sci_b(uint32_t a) {
	return __builtin_corev_and_sc_b(a, 5);
}

// CHECK-LABEL: @test_abs_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.abs.h(i32 [[TMP0]])
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_abs_h(uint32_t a) {
	return __builtin_corev_abs_h(a);
}

// CHECK-LABEL: @test_abs_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.abs.b(i32 [[TMP0]])
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_abs_b(uint32_t a) {
	return __builtin_corev_abs_b(a);
}

// CHECK-LABEL: @test_dotup_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.dotup.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_dotup_h(uint32_t a, uint32_t b) {
	return __builtin_corev_dotup_h(a, b);
}

// CHECK-LABEL: @test_dotup_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.dotup.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_dotup_b(uint32_t a, uint32_t b) {
	return __builtin_corev_dotup_b(a, b);
}

// CHECK-LABEL: @test_dotup_sc_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.dotup.sc.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_dotup_sc_h(uint32_t a, uint32_t b) {
	return __builtin_corev_dotup_sc_h(a, b);
}

// CHECK-LABEL: @test_dotup_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.dotup.sc.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_dotup_sci_h(uint32_t a) {
	return __builtin_corev_dotup_sc_h(a, 5);
}

// CHECK-LABEL: @test_dotup_sc_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.dotup.sc.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_dotup_sc_b(uint32_t a, uint32_t b) {
	return __builtin_corev_dotup_sc_b(a, b);
}

// CHECK-LABEL: @test_dotup_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.dotup.sc.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_dotup_sci_b(uint32_t a) {
	return __builtin_corev_dotup_sc_b(a, 5);
}

// CHECK-LABEL: @test_dotusp_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.dotusp.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_dotusp_h(uint32_t a, uint32_t b) {
	return __builtin_corev_dotusp_h(a, b);
}

// CHECK-LABEL: @test_dotusp_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.dotusp.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_dotusp_b(uint32_t a, uint32_t b) {
	return __builtin_corev_dotusp_b(a, b);
}

// CHECK-LABEL: @test_dotusp_sc_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.dotusp.sc.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_dotusp_sc_h(uint32_t a, uint32_t b) {
	return __builtin_corev_dotusp_sc_h(a, b);
}

// CHECK-LABEL: @test_dotusp_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.dotusp.sc.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_dotusp_sci_h(uint32_t a) {
	return __builtin_corev_dotusp_sc_h(a, 5);
}

// CHECK-LABEL: @test_dotusp_sc_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.dotusp.sc.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_dotusp_sc_b(uint32_t a, uint32_t b) {
	return __builtin_corev_dotusp_sc_b(a, b);
}

// CHECK-LABEL: @test_dotusp_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.dotusp.sc.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_dotusp_sci_b(uint32_t a) {
	return __builtin_corev_dotusp_sc_b(a, 5);
}

// CHECK-LABEL: @test_dotsp_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.dotsp.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_dotsp_h(uint32_t a, uint32_t b) {
	return __builtin_corev_dotsp_h(a, b);
}

// CHECK-LABEL: @test_dotsp_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.dotsp.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_dotsp_b(uint32_t a, uint32_t b) {
	return __builtin_corev_dotsp_b(a, b);
}

// CHECK-LABEL: @test_dotsp_sc_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.dotsp.sc.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_dotsp_sc_h(uint32_t a, uint32_t b) {
	return __builtin_corev_dotsp_sc_h(a, b);
}

// CHECK-LABEL: @test_dotsp_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.dotsp.sc.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_dotsp_sci_h(uint32_t a) {
	return __builtin_corev_dotsp_sc_h(a, 5);
}

// CHECK-LABEL: @test_dotsp_sc_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.dotsp.sc.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_dotsp_sc_b(uint32_t a, uint32_t b) {
	return __builtin_corev_dotsp_sc_b(a, b);
}

// CHECK-LABEL: @test_dotsp_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.dotsp.sc.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_dotsp_sci_b(uint32_t a) {
	return __builtin_corev_dotsp_sc_b(a, 5);
}

// CHECK-LABEL: @test_sdotup_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.sdotup.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_sdotup_h(uint32_t a, uint32_t b) {
	return __builtin_corev_sdotup_h(a, b);
}

// CHECK-LABEL: @test_sdotup_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.sdotup.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_sdotup_b(uint32_t a, uint32_t b) {
	return __builtin_corev_sdotup_b(a, b);
}

// CHECK-LABEL: @test_sdotup_sc_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.sdotup.sc.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_sdotup_sc_h(uint32_t a, uint32_t b) {
	return __builtin_corev_sdotup_sc_h(a, b);
}

// CHECK-LABEL: @test_sdotup_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.sdotup.sc.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_sdotup_sci_h(uint32_t a) {
	return __builtin_corev_sdotup_sc_h(a, 5);
}

// CHECK-LABEL: @test_sdotup_sc_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.sdotup.sc.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_sdotup_sc_b(uint32_t a, uint32_t b) {
	return __builtin_corev_sdotup_sc_b(a, b);
}

// CHECK-LABEL: @test_sdotup_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.sdotup.sc.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_sdotup_sci_b(uint32_t a) {
	return __builtin_corev_sdotup_sc_b(a, 5);
}

// CHECK-LABEL: @test_sdotusp_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.sdotusp.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_sdotusp_h(uint32_t a, uint32_t b) {
	return __builtin_corev_sdotusp_h(a, b);
}

// CHECK-LABEL: @test_sdotusp_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.sdotusp.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_sdotusp_b(uint32_t a, uint32_t b) {
	return __builtin_corev_sdotusp_b(a, b);
}

// CHECK-LABEL: @test_sdotusp_sc_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.sdotusp.sc.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_sdotusp_sc_h(uint32_t a, uint32_t b) {
	return __builtin_corev_sdotusp_sc_h(a, b);
}

// CHECK-LABEL: @test_sdotusp_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.sdotusp.sc.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_sdotusp_sci_h(uint32_t a) {
	return __builtin_corev_sdotusp_sc_h(a, 5);
}

// CHECK-LABEL: @test_sdotusp_sc_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.sdotusp.sc.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_sdotusp_sc_b(uint32_t a, uint32_t b) {
	return __builtin_corev_sdotusp_sc_b(a, b);
}

// CHECK-LABEL: @test_sdotusp_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.sdotusp.sc.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_sdotusp_sci_b(uint32_t a) {
	return __builtin_corev_sdotusp_sc_b(a, 5);
}

// CHECK-LABEL: @test_sdotsp_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.sdotsp.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_sdotsp_h(uint32_t a, uint32_t b) {
	return __builtin_corev_sdotsp_h(a, b);
}

// CHECK-LABEL: @test_sdotsp_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.sdotsp.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_sdotsp_b(uint32_t a, uint32_t b) {
	return __builtin_corev_sdotsp_b(a, b);
}

// CHECK-LABEL: @test_sdotsp_sc_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.sdotsp.sc.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_sdotsp_sc_h(uint32_t a, uint32_t b) {
	return __builtin_corev_sdotsp_sc_h(a, b);
}

// CHECK-LABEL: @test_sdotsp_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.sdotsp.sc.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_sdotsp_sci_h(uint32_t a) {
	return __builtin_corev_sdotsp_sc_h(a, 5);
}

// CHECK-LABEL: @test_sdotsp_sc_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.sdotsp.sc.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_sdotsp_sc_b(uint32_t a, uint32_t b) {
	return __builtin_corev_sdotsp_sc_b(a, b);
}

// CHECK-LABEL: @test_sdotsp_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.sdotsp.sc.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_sdotsp_sci_b(uint32_t a) {
	return __builtin_corev_sdotsp_sc_b(a, 5);
}

// CHECK-LABEL: @test_extract_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.extract.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_extract_h(uint32_t a) {
	return __builtin_corev_extract_h(a, 5);
}

// CHECK-LABEL: @test_extract_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.extract.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_extract_b(uint32_t a) {
	return __builtin_corev_extract_b(a, 5);
}

// CHECK-LABEL: @test_extractu_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.extractu.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_extractu_h(uint32_t a) {
	return __builtin_corev_extractu_h(a, 5);
}

// CHECK-LABEL: @test_extractu_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.extractu.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_extractu_b(uint32_t a) {
	return __builtin_corev_extractu_b(a, 5);
}

// CHECK-LABEL: @test_insert_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.insert.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_insert_h(uint32_t a) {
	return __builtin_corev_insert_h(a, 5);
}

// CHECK-LABEL: @test_insert_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.insert.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_insert_b(uint32_t a) {
	return __builtin_corev_insert_b(a, 5);
}

// CHECK-LABEL: @test_shuffle_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.shuffle.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_shuffle_h(uint32_t a, uint32_t b) {
	return __builtin_corev_shuffle_h(a, b);
}

// CHECK-LABEL: @test_shuffle_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.shuffle.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_shuffle_b(uint32_t a, uint32_t b) {
	return __builtin_corev_shuffle_b(a, b);
}

// CHECK-LABEL: @test_shuffle_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.shuffle.sci.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_shuffle_sci_h(uint32_t a) {
	return __builtin_corev_shuffle_sci_h(a, 5);
}

// CHECK-LABEL: @test_shuffleI0_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.shuffleI0.sci.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_shuffleI0_sci_b(uint32_t a) {
	return __builtin_corev_shuffleI0_sci_b(a, 5);
}

// CHECK-LABEL: @test_shuffleI1_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.shuffleI1.sci.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_shuffleI1_sci_b(uint32_t a) {
	return __builtin_corev_shuffleI1_sci_b(a, 5);
}

// CHECK-LABEL: @test_shuffleI2_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.shuffleI2.sci.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_shuffleI2_sci_b(uint32_t a) {
	return __builtin_corev_shuffleI2_sci_b(a, 5);
}

// CHECK-LABEL: @test_shuffleI3_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.shuffleI3.sci.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_shuffleI3_sci_b(uint32_t a) {
	return __builtin_corev_shuffleI3_sci_b(a, 5);
}

// CHECK-LABEL: @test_shuffle2_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.shuffle2.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_shuffle2_h(uint32_t a, uint32_t b) {
	return __builtin_corev_shuffle2_h(a, b);
}

// CHECK-LABEL: @test_shuffle2_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.shuffle2.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_shuffle2_b(uint32_t a, uint32_t b) {
	return __builtin_corev_shuffle2_b(a, b);
}

// CHECK-LABEL: @test_pack(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.pack(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_pack(uint32_t a, uint32_t b) {
	return __builtin_corev_pack(a, b);
}

// CHECK-LABEL: @test_pack_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.pack.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_pack_h(uint32_t a, uint32_t b) {
	return __builtin_corev_pack_h(a, b);
}

// CHECK-LABEL: @test_packhi_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.packhi.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_packhi_b(uint32_t a, uint32_t b) {
	return __builtin_corev_packhi_b(a, b);
}

// CHECK-LABEL: @test_packlo_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.packlo.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_packlo_b(uint32_t a, uint32_t b) {
	return __builtin_corev_packlo_b(a, b);
}

// CHECK-LABEL: @test_cmpeq_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.cmpeq.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cmpeq_h(uint32_t a, uint32_t b) {
	return __builtin_corev_cmpeq_h(a, b);
}

// CHECK-LABEL: @test_cmpeq_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.cmpeq.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cmpeq_b(uint32_t a, uint32_t b) {
	return __builtin_corev_cmpeq_b(a, b);
}

// CHECK-LABEL: @test_cmpeq_sc_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.cmpeq.sc.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cmpeq_sc_h(uint32_t a, uint32_t b) {
	return __builtin_corev_cmpeq_sc_h(a, b);
}

// CHECK-LABEL: @test_cmpeq_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.cmpeq.sc.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_cmpeq_sci_h(uint32_t a) {
	return __builtin_corev_cmpeq_sc_h(a, 5);
}

// CHECK-LABEL: @test_cmpeq_sc_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.cmpeq.sc.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cmpeq_sc_b(uint32_t a, uint32_t b) {
	return __builtin_corev_cmpeq_sc_b(a, b);
}

// CHECK-LABEL: @test_cmpeq_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.cmpeq.sc.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_cmpeq_sci_b(uint32_t a) {
	return __builtin_corev_cmpeq_sc_b(a, 5);
}

// CHECK-LABEL: @test_cmpne_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.cmpne.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cmpne_h(uint32_t a, uint32_t b) {
	return __builtin_corev_cmpne_h(a, b);
}

// CHECK-LABEL: @test_cmpne_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.cmpne.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cmpne_b(uint32_t a, uint32_t b) {
	return __builtin_corev_cmpne_b(a, b);
}

// CHECK-LABEL: @test_cmpne_sc_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.cmpne.sc.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cmpne_sc_h(uint32_t a, uint32_t b) {
	return __builtin_corev_cmpne_sc_h(a, b);
}

// CHECK-LABEL: @test_cmpne_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.cmpne.sc.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_cmpne_sci_h(uint32_t a) {
	return __builtin_corev_cmpne_sc_h(a, 5);
}

// CHECK-LABEL: @test_cmpne_sc_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.cmpne.sc.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cmpne_sc_b(uint32_t a, uint32_t b) {
	return __builtin_corev_cmpne_sc_b(a, b);
}

// CHECK-LABEL: @test_cmpne_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.cmpne.sc.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_cmpne_sci_b(uint32_t a) {
	return __builtin_corev_cmpne_sc_b(a, 5);
}

// CHECK-LABEL: @test_cmpgt_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.cmpgt.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cmpgt_h(uint32_t a, uint32_t b) {
	return __builtin_corev_cmpgt_h(a, b);
}

// CHECK-LABEL: @test_cmpgt_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.cmpgt.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cmpgt_b(uint32_t a, uint32_t b) {
	return __builtin_corev_cmpgt_b(a, b);
}

// CHECK-LABEL: @test_cmpgt_sc_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.cmpgt.sc.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cmpgt_sc_h(uint32_t a, uint32_t b) {
	return __builtin_corev_cmpgt_sc_h(a, b);
}

// CHECK-LABEL: @test_cmpgt_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.cmpgt.sc.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_cmpgt_sci_h(uint32_t a) {
	return __builtin_corev_cmpgt_sc_h(a, 5);
}

// CHECK-LABEL: @test_cmpgt_sc_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.cmpgt.sc.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cmpgt_sc_b(uint32_t a, uint32_t b) {
	return __builtin_corev_cmpgt_sc_b(a, b);
}

// CHECK-LABEL: @test_cmpgt_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.cmpgt.sc.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_cmpgt_sci_b(uint32_t a) {
	return __builtin_corev_cmpgt_sc_b(a, 5);
}

// CHECK-LABEL: @test_cmpge_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.cmpge.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cmpge_h(uint32_t a, uint32_t b) {
	return __builtin_corev_cmpge_h(a, b);
}

// CHECK-LABEL: @test_cmpge_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.cmpge.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cmpge_b(uint32_t a, uint32_t b) {
	return __builtin_corev_cmpge_b(a, b);
}

// CHECK-LABEL: @test_cmpge_sc_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.cmpge.sc.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cmpge_sc_h(uint32_t a, uint32_t b) {
	return __builtin_corev_cmpge_sc_h(a, b);
}

// CHECK-LABEL: @test_cmpge_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.cmpge.sc.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_cmpge_sci_h(uint32_t a) {
	return __builtin_corev_cmpge_sc_h(a, 5);
}

// CHECK-LABEL: @test_cmpge_sc_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.cmpge.sc.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cmpge_sc_b(uint32_t a, uint32_t b) {
	return __builtin_corev_cmpge_sc_b(a, b);
}

// CHECK-LABEL: @test_cmpge_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.cmpge.sc.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_cmpge_sci_b(uint32_t a) {
	return __builtin_corev_cmpge_sc_b(a, 5);
}

// CHECK-LABEL: @test_cmplt_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.cmplt.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cmplt_h(uint32_t a, uint32_t b) {
	return __builtin_corev_cmplt_h(a, b);
}

// CHECK-LABEL: @test_cmplt_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.cmplt.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cmplt_b(uint32_t a, uint32_t b) {
	return __builtin_corev_cmplt_b(a, b);
}

// CHECK-LABEL: @test_cmplt_sc_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.cmplt.sc.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cmplt_sc_h(uint32_t a, uint32_t b) {
	return __builtin_corev_cmplt_sc_h(a, b);
}

// CHECK-LABEL: @test_cmplt_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.cmplt.sc.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_cmplt_sci_h(uint32_t a) {
	return __builtin_corev_cmplt_sc_h(a, 5);
}

// CHECK-LABEL: @test_cmplt_sc_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.cmplt.sc.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cmplt_sc_b(uint32_t a, uint32_t b) {
	return __builtin_corev_cmplt_sc_b(a, b);
}

// CHECK-LABEL: @test_cmplt_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.cmplt.sc.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_cmplt_sci_b(uint32_t a) {
	return __builtin_corev_cmplt_sc_b(a, 5);
}

// CHECK-LABEL: @test_cmple_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.cmple.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cmple_h(uint32_t a, uint32_t b) {
	return __builtin_corev_cmple_h(a, b);
}

// CHECK-LABEL: @test_cmple_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.cmple.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cmple_b(uint32_t a, uint32_t b) {
	return __builtin_corev_cmple_b(a, b);
}

// CHECK-LABEL: @test_cmple_sc_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.cmple.sc.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cmple_sc_h(uint32_t a, uint32_t b) {
	return __builtin_corev_cmple_sc_h(a, b);
}

// CHECK-LABEL: @test_cmple_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.cmple.sc.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_cmple_sci_h(uint32_t a) {
	return __builtin_corev_cmple_sc_h(a, 5);
}

// CHECK-LABEL: @test_cmple_sc_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.cmple.sc.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cmple_sc_b(uint32_t a, uint32_t b) {
	return __builtin_corev_cmple_sc_b(a, b);
}

// CHECK-LABEL: @test_cmple_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.cmple.sc.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_cmple_sci_b(uint32_t a) {
	return __builtin_corev_cmple_sc_b(a, 5);
}

// CHECK-LABEL: @test_cmpgtu_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.cmpgtu.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cmpgtu_h(uint32_t a, uint32_t b) {
	return __builtin_corev_cmpgtu_h(a, b);
}

// CHECK-LABEL: @test_cmpgtu_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.cmpgtu.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cmpgtu_b(uint32_t a, uint32_t b) {
	return __builtin_corev_cmpgtu_b(a, b);
}

// CHECK-LABEL: @test_cmpgtu_sc_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.cmpgtu.sc.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cmpgtu_sc_h(uint32_t a, uint32_t b) {
	return __builtin_corev_cmpgtu_sc_h(a, b);
}

// CHECK-LABEL: @test_cmpgtu_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.cmpgtu.sc.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_cmpgtu_sci_h(uint32_t a) {
	return __builtin_corev_cmpgtu_sc_h(a, 5);
}

// CHECK-LABEL: @test_cmpgtu_sc_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.cmpgtu.sc.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cmpgtu_sc_b(uint32_t a, uint32_t b) {
	return __builtin_corev_cmpgtu_sc_b(a, b);
}

// CHECK-LABEL: @test_cmpgtu_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.cmpgtu.sc.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_cmpgtu_sci_b(uint32_t a) {
	return __builtin_corev_cmpgtu_sc_b(a, 5);
}

// CHECK-LABEL: @test_cmpgeu_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.cmpgeu.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cmpgeu_h(uint32_t a, uint32_t b) {
	return __builtin_corev_cmpgeu_h(a, b);
}

// CHECK-LABEL: @test_cmpgeu_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.cmpgeu.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cmpgeu_b(uint32_t a, uint32_t b) {
	return __builtin_corev_cmpgeu_b(a, b);
}

// CHECK-LABEL: @test_cmpgeu_sc_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.cmpgeu.sc.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cmpgeu_sc_h(uint32_t a, uint32_t b) {
	return __builtin_corev_cmpgeu_sc_h(a, b);
}

// CHECK-LABEL: @test_cmpgeu_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.cmpgeu.sc.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_cmpgeu_sci_h(uint32_t a) {
	return __builtin_corev_cmpgeu_sc_h(a, 5);
}

// CHECK-LABEL: @test_cmpgeu_sc_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.cmpgeu.sc.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cmpgeu_sc_b(uint32_t a, uint32_t b) {
	return __builtin_corev_cmpgeu_sc_b(a, b);
}

// CHECK-LABEL: @test_cmpgeu_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.cmpgeu.sc.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_cmpgeu_sci_b(uint32_t a) {
	return __builtin_corev_cmpgeu_sc_b(a, 5);
}

// CHECK-LABEL: @test_cmpltu_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.cmpltu.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cmpltu_h(uint32_t a, uint32_t b) {
	return __builtin_corev_cmpltu_h(a, b);
}

// CHECK-LABEL: @test_cmpltu_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.cmpltu.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cmpltu_b(uint32_t a, uint32_t b) {
	return __builtin_corev_cmpltu_b(a, b);
}

// CHECK-LABEL: @test_cmpltu_sc_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.cmpltu.sc.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cmpltu_sc_h(uint32_t a, uint32_t b) {
	return __builtin_corev_cmpltu_sc_h(a, b);
}

// CHECK-LABEL: @test_cmpltu_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.cmpltu.sc.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_cmpltu_sci_h(uint32_t a) {
	return __builtin_corev_cmpltu_sc_h(a, 5);
}

// CHECK-LABEL: @test_cmpltu_sc_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.cmpltu.sc.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cmpltu_sc_b(uint32_t a, uint32_t b) {
	return __builtin_corev_cmpltu_sc_b(a, b);
}

// CHECK-LABEL: @test_cmpltu_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.cmpltu.sc.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_cmpltu_sci_b(uint32_t a) {
	return __builtin_corev_cmpltu_sc_b(a, 5);
}

// CHECK-LABEL: @test_cmpleu_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.cmpleu.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cmpleu_h(uint32_t a, uint32_t b) {
	return __builtin_corev_cmpleu_h(a, b);
}

// CHECK-LABEL: @test_cmpleu_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.cmpleu.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cmpleu_b(uint32_t a, uint32_t b) {
	return __builtin_corev_cmpleu_b(a, b);
}

// CHECK-LABEL: @test_cmpleu_sc_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.cmpleu.sc.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cmpleu_sc_h(uint32_t a, uint32_t b) {
	return __builtin_corev_cmpleu_sc_h(a, b);
}

// CHECK-LABEL: @test_cmpleu_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.cmpleu.sc.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_cmpleu_sci_h(uint32_t a) {
	return __builtin_corev_cmpleu_sc_h(a, 5);
}

// CHECK-LABEL: @test_cmpleu_sc_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.cmpleu.sc.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cmpleu_sc_b(uint32_t a, uint32_t b) {
	return __builtin_corev_cmpleu_sc_b(a, b);
}

// CHECK-LABEL: @test_cmpleu_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.cmpleu.sc.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_cmpleu_sci_b(uint32_t a) {
	return __builtin_corev_cmpleu_sc_b(a, 5);
}

// CHECK-LABEL: @test_cplxmul_r(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.cplxmul.r(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cplxmul_r(uint32_t a, uint32_t b) {
	return __builtin_corev_cplxmul_r(a, b);
}

// CHECK-LABEL: @test_cplxmul_i(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.cplxmul.i(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cplxmul_i(uint32_t a, uint32_t b) {
	return __builtin_corev_cplxmul_i(a, b);
}

// CHECK-LABEL: @test_cplxmul_r_div2(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.cplxmul.r.div2(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cplxmul_r_div2(uint32_t a, uint32_t b) {
	return __builtin_corev_cplxmul_r_div2(a, b);
}

// CHECK-LABEL: @test_cplxmul_i_div2(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.cplxmul.i.div2(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cplxmul_i_div2(uint32_t a, uint32_t b) {
	return __builtin_corev_cplxmul_i_div2(a, b);
}

// CHECK-LABEL: @test_cplxmul_r_div4(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.cplxmul.r.div4(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cplxmul_r_div4(uint32_t a, uint32_t b) {
	return __builtin_corev_cplxmul_r_div4(a, b);
}

// CHECK-LABEL: @test_cplxmul_i_div4(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.cplxmul.i.div4(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cplxmul_i_div4(uint32_t a, uint32_t b) {
	return __builtin_corev_cplxmul_i_div4(a, b);
}

// CHECK-LABEL: @test_cplxmul_r_div8(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.cplxmul.r.div8(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cplxmul_r_div8(uint32_t a, uint32_t b) {
	return __builtin_corev_cplxmul_r_div8(a, b);
}

// CHECK-LABEL: @test_cplxmul_i_div8(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.cplxmul.i.div8(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cplxmul_i_div8(uint32_t a, uint32_t b) {
	return __builtin_corev_cplxmul_i_div8(a, b);
}

// CHECK-LABEL: @test_cplxconj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.cplxconj(i32 [[TMP0]])
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_cplxconj(uint32_t a) {
	return __builtin_corev_cplxconj(a);
}

// CHECK-LABEL: @test_subrotmj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.subrotmj(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_subrotmj(uint32_t a, uint32_t b) {
	return __builtin_corev_subrotmj(a, b);
}

// CHECK-LABEL: @test_subrotmj_div2(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.subrotmj.div2(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_subrotmj_div2(uint32_t a, uint32_t b) {
	return __builtin_corev_subrotmj_div2(a, b);
}

// CHECK-LABEL: @test_subrotmj_div4(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.subrotmj.div4(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_subrotmj_div4(uint32_t a, uint32_t b) {
	return __builtin_corev_subrotmj_div4(a, b);
}

// CHECK-LABEL: @test_subrotmj_div8(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.subrotmj.div8(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_subrotmj_div8(uint32_t a, uint32_t b) {
	return __builtin_corev_subrotmj_div8(a, b);
}

// CHECK-LABEL: @test_add_div2(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.add.div2(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_add_div2(uint32_t a, uint32_t b) {
	return __builtin_corev_add_div2(a, b);
}

// CHECK-LABEL: @test_add_div4(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.add.div4(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_add_div4(uint32_t a, uint32_t b) {
	return __builtin_corev_add_div4(a, b);
}

// CHECK-LABEL: @test_add_div8(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.add.div8(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_add_div8(uint32_t a, uint32_t b) {
	return __builtin_corev_add_div8(a, b);
}

// CHECK-LABEL: @test_sub_div2(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.sub.div2(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_sub_div2(uint32_t a, uint32_t b) {
	return __builtin_corev_sub_div2(a, b);
}

// CHECK-LABEL: @test_sub_div4(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.sub.div4(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_sub_div4(uint32_t a, uint32_t b) {
	return __builtin_corev_sub_div4(a, b);
}

// CHECK-LABEL: @test_sub_div8(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.sub.div8(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_sub_div8(uint32_t a, uint32_t b) {
	return __builtin_corev_sub_div8(a, b);
}

